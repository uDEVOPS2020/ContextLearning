
## Use case 2. Characterizing causal-effect releations betweeen microservices in an MSA. 
The folder "Case 2" contains: 
- **Dataset**. This is the dataset used as illustrative example for this use case. It is derived by running a load on a well-known open-source benchmark for microservice  architecture (MSA), named Train Ticket [1].  The application simulates a train ticket booking system, composed of 41 microservices communicating to each other via REST over HTTP. Train ticket is  polyglot (e.g., Java, golang, Node.js, etc). 

Dataset has the following columns: 

@LUCA    ... | Reponse Time MS1 | ... | Response Time MSn |    (LE COLONNE con metriche di container le tagliamo)

Each row is a sample every 5 seconds. 

- **Workload generation**. The dataset are generated by stressing the system with a workload. The folder contains the files to use and customise the tool Locust [2], which we exploited to generate the load. Locust is a distributed, open-source load testing tool that simulates concurrent users in an application for each benchmark. We customize the workload to reflect the behavior of real users. In total, we run XXXXXX queries per second for XXX time.  

- **Monitoring data**. The folder contains raw data gathered by monitoring. 
This includes: ..xxxxx,.   xxxxx,. xxxxx. 
Monitoring data are collected by Locust. 

- *Benchmark.txt*. The file contains a link to the MSA under analysis.  

- **Results**. It contains the results of applying the model to the dataset. 

- **Code**. Python code files/scripts to: i) gather monitoring data, ii) parse the raw monitoring data and creating the dataset, iii) read the dataset and apply the algorithms for causal structure discovery to create a causal model, iv) rank the services in terms of most probable root cause for the observed performance behaviour, and v) present the result.  

To reproduce:  use the code files from steps iii) to v). 

*Prerequisites*: 
....

*Commands*: 
./apply_causal_model.py dataset.txt
./rank_and_present.py
 
To replicate on a different subject, use Locust and monitoring scripts on the new subject, thus apply all the steps i) to v). 

*Commands*:
... 
...


## Use case 3 

Description (DA SPOSTARE IN DELIVEARBLE, no va qua): 
Opzione 1. Backward
The goal is to setup a solution for energy consumption anomaly and/or changepoint detection at node level, and then identify the microservice contributing more to it. This can then be extended to every node. Techniques: Forecast-based (Prophet, STL, GESD, IQR.), followed by multivariate transfer entropy (MuTE) to spot most impacting containers. 

Opzione 2. Forward. 
Same goal, but the idea is to build a multivariate model for forecasting (Vector Arima), with the continuer metrics along with the consumption metric, and then use feature selection/ranking to identify the culprit microservice. 


The folder "Case 3" contains: 
- **Dataset**. This is the dataset used as illustrative example for this use case. It is derived by running a load on a well-known open-source benchmark for microservice  architecture (MSA), named Train Ticket [1].  The application simulates a train ticket booking system, composed of 41 microservices communicating to each other via REST over HTTP. Train ticket is  polyglot (e.g., Java, golang, Node.js, etc). 
Dataset has the following columns: 
 ... | CPU Node Consumption | Memory Node Consumption | ..AltreNodeLevel ...| ... |Container1CPU | ... |ContainerN CPU|....|Container1Memory | ... |ContainerN Memory| ...|Altre? |


Each row is a sample every 5 seconds. 

- **Workload generation**. The dataset are generated by stressing the system with a workload. The folder contains the files to use and customise the tool Locust [2], which we exploited to generate the load. Locust is a distributed, open-source load testing tool that simulates concurrent users in an application for each benchmark. We customize the workload to reflect the behavior of real users. In total, we run XXXXXX queries per second for XXX time.  

- *Benchmark.txt*. The file contains a link to the MSA under analysis.  
 
- **Results**. It contains the results of applying the model to the dataset. 

- **Code**. Python code files/scripts to: i) gather monitoring data, ii) parse the raw monitoring data and creating the dataset, iii) read the dataset and apply the algorithms to create the model for anomaly/changepoint detection and then identify the top-X microservices impacting more node-level stress/consumption, iv) present the result.  

To reproduce:  use the code files from steps iii) to iv). 

*Prerequisites*: 
....

*Commands*: 

./apply_anomaly_detection.py dataset.txt #for anomaly detection
./apply_MuTE_model.py dataset.txt	 #for modelling relation between microservices and the anomaly
./rank_and_present.py			 #for ranking and presenting results
 
To replicate on a different subject, use Locust and monitoring scripts on the new subject, thus apply all the steps i) to iv). 

*Commands*:
... 
...


## Use case 4 

The folder "Case 4" contains: 
- **Dataset**. This is the dataset used as illustrative example for this use case. The dataset consists of commit data for 6 (VA BENE ANCHE UNA) open source applications. It is cloned from a Just-In-Time Defect Prediction (JIT-DP) research work repository [1]. The generation procedure is as follows: PROCEDURA DA PAPER: data extraction, labelling by the SZZ algorithm.

Each subject dataset has the following columns: 
 ... |Nome metriche commit | ...  | .. ...| ... |label=Buggy| 

Each row refers to a commit. 

- *Benchmark.txt*. The file contains links to the applications under analysis.  

- **Results**. It contains the results of applying the model to the datasets. 
 
- **Code**. Python code files/scripts to: i) perform defect prediction on a dataset in the same format as the Dataset's folder files; this includes code for training the C2Vec model for JIT-DP and corse-validating results. The files are changed from the original code [1] to add the computation of feature stability. Feature stability is computed according to the algorithm described in [2]. ii) present the results. 

Code to create datasets (hence extracting GitHib metrics and labelling dataset) is available from the reference paper repository [3].  

To reproduce:  use the code files, steps i) and ii). 

*Prerequisites*: 
...

*Commands*: 

./vedi comandi paper 			 #for JIT-DP
./compute_stability			 #for features stability computation 
./rank_and_present.py			 #for ranking and presenting results, from most to least stable metrics
 
To replicate on a different subject, use the code in the repository of the JIT-DP paper we started from [3], then apply steps i) and ii). 

*Commands*:
... 
...
