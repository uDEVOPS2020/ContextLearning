Use case 1
=============
	- Description: The objective of this use case is to support test prioritization, namely: given a list of tests to run, the goal is to run first the ones more likely to expose failures.    
    Prioritization is done by applying machine learning algorithms to features of request/response and/or of the invoked Microservice that correlated more with quality metrics, such as performance (e.g., response time), reliability (e.g., status code) or coverage. The feedback allows for prioritizing test cases.  
		- Source of information: Execution traces of previously executed testing sessions, or resulting from monitoring of the application during operation.
    
		- Metrics: Response Time and status code are both considered to perform the prioritization. The  ranking score is computed as follows: ranking_score = response_code+1/(response_time)*100. The response codes higher than or equal to 400 correspond to failed tests; at the same time, a lower  response time implies a higher  priority of the test. As result, the higher the value of the ranking score, the higher the priority of the considered test. 
    
		- Modeled facet: Behavioural, Failing behaviour
		- Type of model(s): Learning-to-rank algorithm

		- Learning algorithm used: Learning-to-rank (classification/regression) algorithms included in the RankLib library (Dang, V. “The Lemur Project-Wiki-RankLib.” Lemur Project,[Online]. Available at http://sourceforge.net/p/lemur/wiki/RankLib):
			- MART (Multiple Additive Regression Trees, a.k.a. Gradient boosted regression tree)
			- RankNet
			- RankBoost
			- AdaRank
			- Coordinate Ascent
			- LambdaMART
			- ListNet
			- Random Forests

		- Decision support category: Fault avoidance
		- Decision support about: Testing for fault detection and coverage.
