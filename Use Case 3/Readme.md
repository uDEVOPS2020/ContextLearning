## Use case 3. Anomaly detection and root cause analysis 

Description (To move to Overleaf): 
Opzione 1. Backward
The goal is to setup a solution for energy consumption anomaly and/or changepoint detection at node level, and then identify the microservice contributing more to it. This can then be extended to every node. Techniques: Forecast-based (Prophet, STL, GESD, IQR.), followed by multivariate transfer entropy (MuTE) to spot most impacting containers. 

Opzione 2. Forward. 
Same goal, but the idea is to build a multivariate model for forecasting (Vector Arima), with the continuer metrics along with the consumption metric, and then use feature selection/ranking to identify the culprit microservice. 

This folder contains: 
- **Dataset**. This is the dataset used as illustrative example for this use case. It is derived by running a load on a well-known open-source benchmark for microservice  architecture (MSA), named Train Ticket [1].  The application simulates a train ticket booking system, composed of 41 microservices communicating to each other via REST over HTTP. Train ticket is  polyglot (e.g., Java, golang, Node.js, etc). 
Dataset has the following columns: 
|Container1 CPU usage| ... |ContainerN CPU usage|Container1 Memory usage| ... |ContainerN Memory usage|


Each row is a sample every 5 seconds. 

- **Workload generation**. The dataset are generated by stressing the system with a workload. The folder contains the files to use and customise the tool Locust [2], which we exploited to generate the load. Locust is a distributed, open-source load testing tool that simulates concurrent users in an application for each benchmark. We customize the workload to reflect the behavior of real users. In total, we run on average 8 queries per second for 5 minutes, with 10 users. The run configuration foresees a "step" patterns in which the load is increased to 150 users after 5 minutes in order to see to what extent the anomaly is detected. 

- **Monitoring data**. The folder contains raw data gathered by monitoring. This includes: ..//ToAdd. Monitoring data are collected by DockerStats run in each container. 

- *Benchmark.txt*. The file contains a link to the MSA under analysis.  
 
- **Results**. It contains the results of applying the model to the dataset. They show the graphical representation of possible anomalies detected in any container-level metric, and then the result of Multivariate Transfer Entropy (MuTE) applied to the series of the same type (e.g., all CPU consumption time series) in order to assess any temporal cause-effect relation. The MuTE output are both in textaul form and as a graph (i.e., the network) representing which time series transfer more entropy to which other and at which significance.  

- **Code**. Python code files/scripts to: i) gather monitoring data, ii) parse the raw monitoring data and creating the dataset, iii) read the dataset and apply the algorithms to create the model for anomaly/changepoint detection, and iv) then identify the top-X microservices impacting more node-level stress/consumption by applying MuTE. 

To reproduce:  use the code files from steps iii) to iv). 

*Prerequisites*: 

Python (version >3), JVM/JRE (version > 1.8). 

Libraries: Anomalize (R code, https://cran.r-project.org/web/packages/anomalize/)  IDTxl (https://github.com/pwollstadt/IDTxl)

*Commands*: 
> ./anomaly_detection.R   #for anomaly detection

> ./MuTE.py             	 #for modelling relation between microservices and the anomaly
 
To replicate on a different subject, use Locust and DockerStats monitoring on the new subject, thus apply all the steps i) to iv). Replace the the dataset input filenames in the the two scripts above. 




